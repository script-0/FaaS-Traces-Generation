{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQVjOczgND0o"
      },
      "source": [
        "---\n",
        "***Cloud computing project: Generating FAAS traces***\n",
        "\n",
        "School: **National Advanced School of Engineering Yaounde**\n",
        "\n",
        "[ Data ](https://azurecloudpublicdataset2.blob.core.windows.net/azurepublicdatasetv2/azurefunctions_dataset2019/azurefunctions-dataset2019.tar.xz)\n",
        "\n",
        "[ Data description ](https://github.com/Azure/AzurePublicDataset/blob/master/AzureFunctionsDataset2019.md)\n",
        "\n",
        "<!-- [ Data two ](https://raw.githubusercontent.com/Azure/AzurePublicDataset/master/data/AzureFunctionsInvocationTraceForTwoWeeksJan2021.rar) -->\n",
        "\n",
        "[ vm generator ](https://github.com/huaweicloud/trace_generation_rnn) \n",
        "\n",
        "<!-- Realised by: **Oreal CHIMI, Mario MANENGONO, Isaac NDEMA, Sylvain KOUEMO, Franklin TALOM, Duhamel TSOPGNI** -->\n",
        "\n",
        "<!-- Supervisor: **Pr. Alain TCHANA** -->\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF9d5GUkUwA9"
      },
      "source": [
        "# Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abnbTX-aUk5B",
        "outputId": "cc0c34de-c6b6-4c9e-fa3d-f4f9ed94bb94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYUTZDHppAGO"
      },
      "source": [
        "# Libraries Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2v5uAOTpClz"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uhjAeFNpGkd"
      },
      "outputs": [],
      "source": [
        "!pip install statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEYDtbmtpMp3"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW8dDaYSE5OY"
      },
      "source": [
        "# Import of libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3IDugFSE8Ce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJO565prpiTu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo81Xkf7p1WA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from typing import NamedTuple\n",
        "from enum import Enum\n",
        "from abc import ABC, abstractmethod\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "from math import ceil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eos1Xt9EqW7X"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK-DVPwHqaTU"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/'\n",
        "DAYS = 2\n",
        "REPORT = 10\n",
        "NUMBER_OF_FUNCTIONS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-P5euJDPT0W"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(\"traces_for_generator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g24I8GXapg6j"
      },
      "source": [
        "# download of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-fiz4nKpj6S",
        "outputId": "e546f658-ecb9-446b-d1b8-b52381a3d2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-01-31 22:21:20--  https://azurecloudpublicdataset2.blob.core.windows.net/azurepublicdatasetv2/azurefunctions_dataset2019/azurefunctions-dataset2019.tar.xz\n",
            "Resolving azurecloudpublicdataset2.blob.core.windows.net (azurecloudpublicdataset2.blob.core.windows.net)... 20.38.122.100\n",
            "Connecting to azurecloudpublicdataset2.blob.core.windows.net (azurecloudpublicdataset2.blob.core.windows.net)|20.38.122.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 142968140 (136M) [application/octet-stream]\n",
            "Saving to: ‘azurefunctions-dataset2019.tar.xz.2’\n",
            "\n",
            "azurefunctions-data 100%[===================>] 136.34M  24.3MB/s    in 5.1s    \n",
            "\n",
            "2022-01-31 22:21:26 (26.5 MB/s) - ‘azurefunctions-dataset2019.tar.xz.2’ saved [142968140/142968140]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://azurecloudpublicdataset2.blob.core.windows.net/azurepublicdatasetv2/azurefunctions_dataset2019/azurefunctions-dataset2019.tar.xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVPa3YCnptaK",
        "outputId": "eb00d12a-8cc4-4e3a-b80f-75e2fc1e0d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "app_memory_percentiles.anon.d01.csv\n",
            "app_memory_percentiles.anon.d02.csv\n",
            "app_memory_percentiles.anon.d03.csv\n",
            "app_memory_percentiles.anon.d04.csv\n",
            "app_memory_percentiles.anon.d05.csv\n",
            "app_memory_percentiles.anon.d06.csv\n",
            "app_memory_percentiles.anon.d07.csv\n",
            "app_memory_percentiles.anon.d08.csv\n",
            "app_memory_percentiles.anon.d09.csv\n",
            "app_memory_percentiles.anon.d10.csv\n",
            "app_memory_percentiles.anon.d11.csv\n",
            "app_memory_percentiles.anon.d12.csv\n",
            "function_durations_percentiles.anon.d01.csv\n",
            "function_durations_percentiles.anon.d02.csv\n",
            "function_durations_percentiles.anon.d03.csv\n",
            "function_durations_percentiles.anon.d04.csv\n",
            "function_durations_percentiles.anon.d05.csv\n",
            "function_durations_percentiles.anon.d06.csv\n",
            "function_durations_percentiles.anon.d07.csv\n",
            "function_durations_percentiles.anon.d08.csv\n",
            "function_durations_percentiles.anon.d09.csv\n",
            "function_durations_percentiles.anon.d10.csv\n",
            "function_durations_percentiles.anon.d11.csv\n",
            "function_durations_percentiles.anon.d12.csv\n",
            "function_durations_percentiles.anon.d13.csv\n",
            "function_durations_percentiles.anon.d14.csv\n",
            "invocations_per_function_md.anon.d01.csv\n",
            "invocations_per_function_md.anon.d02.csv\n",
            "invocations_per_function_md.anon.d03.csv\n",
            "invocations_per_function_md.anon.d04.csv\n",
            "invocations_per_function_md.anon.d05.csv\n",
            "invocations_per_function_md.anon.d06.csv\n",
            "invocations_per_function_md.anon.d07.csv\n",
            "invocations_per_function_md.anon.d08.csv\n",
            "invocations_per_function_md.anon.d09.csv\n",
            "invocations_per_function_md.anon.d10.csv\n",
            "invocations_per_function_md.anon.d11.csv\n",
            "invocations_per_function_md.anon.d12.csv\n",
            "invocations_per_function_md.anon.d13.csv\n",
            "invocations_per_function_md.anon.d14.csv\n"
          ]
        }
      ],
      "source": [
        "!tar xvf azurefunctions-dataset2019.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlA0114hKI8V"
      },
      "source": [
        "# Formating Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZK11evDuROi"
      },
      "source": [
        "In order to format the origin dataset, uncomment the imbricated for loop, i.e for minute in ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbHoF-O5AD3s"
      },
      "outputs": [],
      "source": [
        "hash_functions = []\n",
        "\n",
        "for day in range(1, 1+DAYS):\n",
        "  inv_df_d = pd.read_csv(DATA_PATH + 'invocations_per_function_md.anon.d0'+ str(day) +'.csv') if day < 10 else pd.read_csv(DATA_PATH + 'invocations_per_function_md.anon.d'+ str(day) +'.csv')\n",
        "\n",
        "  hash_functions.extend(inv_df_d['HashFunction'].unique().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEeIHhkMU7I5"
      },
      "outputs": [],
      "source": [
        "hash_functions = list(set(hash_functions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e40hwABqG1jV"
      },
      "outputs": [],
      "source": [
        "hash_functions =  hash_functions[:NUMBER_OF_FUNCTIONS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA-rnVbnWnzW"
      },
      "outputs": [],
      "source": [
        "functions = []\n",
        "apps = []\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for day in range(1, 1+DAYS):\n",
        "  inv_df_d = pd.read_csv(DATA_PATH + 'invocations_per_function_md.anon.d0'+ str(day) +'.csv') if day < 10 else pd.read_csv(DATA_PATH + 'invocations_per_function_md.anon.d'+ str(day) +'.csv')\n",
        "  inv_df_d = inv_df_d[inv_df_d['HashFunction'].isin(hash_functions)]\n",
        "\n",
        "  for minute in range(1, 1441):\n",
        "    hashes = inv_df_d.loc[inv_df_d[str(minute)] > 0, ['HashApp', 'HashFunction']]\n",
        "    \n",
        "    x_data = [0]*(1440+DAYS)\n",
        "    x_data[minute-1] = 1\n",
        "    x_data[1440:1440+day] = [1]*day\n",
        "    X_data.append(x_data)\n",
        "\n",
        "    app_func = hashes.groupby('HashApp')\n",
        "    hash_apps = hashes['HashApp'].unique().tolist()\n",
        "    hash_apps_size = len(hash_apps)\n",
        "    \n",
        "    functions.append(',|,'.join(map(str, app_func['HashFunction'].transform(lambda x: ','.join(map(str, x))).unique().tolist())) + ',|')\n",
        "\n",
        "\n",
        "    apps.append(','.join(map(str, hash_apps)))\n",
        "    y_data.append(hash_apps_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RC9cOBdsep5"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(X_data, columns=[str(i) for i in range(1, 1441+DAYS)])\n",
        "data['HashFunction'] = functions\n",
        "data['HashApps'] = apps\n",
        "data['number_of_apps'] = y_data\n",
        "data.to_csv(DATA_PATH + 'data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfgmQyv34hQu"
      },
      "outputs": [],
      "source": [
        "hash_functions.append('|')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYTzbdXfgdEB"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YS4Q85SwAk5"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(DATA_PATH + 'data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rcAeA_I3wOFI",
        "outputId": "e38b0db3-f036-489d-9ac9-a39224391770"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-741871c4-652c-4a82-8677-0fd53fbecb74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1406</th>\n",
              "      <th>1407</th>\n",
              "      <th>1408</th>\n",
              "      <th>1409</th>\n",
              "      <th>1410</th>\n",
              "      <th>1411</th>\n",
              "      <th>1412</th>\n",
              "      <th>1413</th>\n",
              "      <th>1414</th>\n",
              "      <th>1415</th>\n",
              "      <th>1416</th>\n",
              "      <th>1417</th>\n",
              "      <th>1418</th>\n",
              "      <th>1419</th>\n",
              "      <th>1420</th>\n",
              "      <th>1421</th>\n",
              "      <th>1422</th>\n",
              "      <th>1423</th>\n",
              "      <th>1424</th>\n",
              "      <th>1425</th>\n",
              "      <th>1426</th>\n",
              "      <th>1427</th>\n",
              "      <th>1428</th>\n",
              "      <th>1429</th>\n",
              "      <th>1430</th>\n",
              "      <th>1431</th>\n",
              "      <th>1432</th>\n",
              "      <th>1433</th>\n",
              "      <th>1434</th>\n",
              "      <th>1435</th>\n",
              "      <th>1436</th>\n",
              "      <th>1437</th>\n",
              "      <th>1438</th>\n",
              "      <th>1439</th>\n",
              "      <th>1440</th>\n",
              "      <th>1441</th>\n",
              "      <th>1442</th>\n",
              "      <th>HashFunction</th>\n",
              "      <th>HashApps</th>\n",
              "      <th>number_of_apps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>85aeddfb6d84a57fe2a289c40b87f5238a11d6bf127fd4...</td>\n",
              "      <td>f7339562a59677cd37ca76eac05727dda2c1f985440907...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>9beccfa4fa275df7e8df02f3d90bb7e1064991d1e40540...</td>\n",
              "      <td>e27ae721f45879870983ae6b10344d459e4d684a1101b5...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>08a2ef2cfa56c5daf7a6141f0b624fc4b722dfc1624897...</td>\n",
              "      <td>e6504b3dab20436ac11311d92449a49e49db20283937ba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7023333efef552c6c92329fc14880b9e8bfb874eccd1ee...</td>\n",
              "      <td>2d992e044b62c22da716305cbd34007f50be389f956575...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a1925b5b876a43f6a05a9d8dcfad1e981a2f165410f3d2...</td>\n",
              "      <td>9ae3a30807a6b9f0fec3395e0f99f25c1abb205063a8f6...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1445 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-741871c4-652c-4a82-8677-0fd53fbecb74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-741871c4-652c-4a82-8677-0fd53fbecb74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-741871c4-652c-4a82-8677-0fd53fbecb74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   1  2  ...                                           HashApps  number_of_apps\n",
              "0  1  0  ...  f7339562a59677cd37ca76eac05727dda2c1f985440907...               6\n",
              "1  0  1  ...  e27ae721f45879870983ae6b10344d459e4d684a1101b5...               2\n",
              "2  0  0  ...  e6504b3dab20436ac11311d92449a49e49db20283937ba...               1\n",
              "3  0  0  ...  2d992e044b62c22da716305cbd34007f50be389f956575...               2\n",
              "4  0  0  ...  9ae3a30807a6b9f0fec3395e0f99f25c1abb205063a8f6...               2\n",
              "\n",
              "[5 rows x 1445 columns]"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlsX8tvZvqNx"
      },
      "source": [
        "# Arrival model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IHHoXX4zfql"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2enHU9LxVbn"
      },
      "outputs": [],
      "source": [
        "POISSON_ALPHA = 1e-2\n",
        "ARRIVAL_MODEL_NAME = 'arrival_model.pkl'\n",
        "NPOISSON_SAMPS = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWHrwnykww0A"
      },
      "source": [
        "## Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RvpRNFhgwFW"
      },
      "outputs": [],
      "source": [
        "X = data.loc[:, [str(i) for i in range(1, 1441+DAYS)]]    # for 12 days - 1452\n",
        "y = data['number_of_apps']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osZKQ5dio7hk"
      },
      "outputs": [],
      "source": [
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "X = X.astype('float64')\n",
        "y = y.astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hWr6Ke6gvzr"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=10, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsjpNukAyPCp"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzE5bx7Ugund"
      },
      "outputs": [],
      "source": [
        "poisson_fit = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit_regularized(alpha=POISSON_ALPHA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiwgEw5Zjwvm"
      },
      "outputs": [],
      "source": [
        "poisson_fit.save(DATA_PATH + ARRIVAL_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW2MRZj4yUYP"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xvhPkGbBFQM"
      },
      "outputs": [],
      "source": [
        "def get_intervals_on_test(poisson_fit):\n",
        "    \"\"\"Get the quantiles by sampling from the Poisson distribution on the\n",
        "    test set.\n",
        "    \"\"\"\n",
        "    all_predict_cnts = []\n",
        "    for idx, minute_day in enumerate(X_test):\n",
        "        if idx > 0 and idx % REPORT == 0:\n",
        "            logger.info(\"Sampling test dataset number %d\", idx)\n",
        "        # Wrap around back to end at day zero:\n",
        "        preds = poisson_fit.predict(minute_day)\n",
        "        \n",
        "        predicted_cnts = preds  # Now the preds are the predictions\n",
        "        predict_cnts_arr = np.array(predicted_cnts)\n",
        "        all_predict_cnts.append(predict_cnts_arr)\n",
        "    predict_arr = np.array(all_predict_cnts)\n",
        "    p05, p50, p95 = get_quantiles(predict_arr, args)\n",
        "    return p05, p50, p95, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DpAOF-Ljwsx"
      },
      "outputs": [],
      "source": [
        "def get_quantiles(predict_arr):\n",
        "    \"\"\"Get prediction quantiles.\"\"\"\n",
        "    all_means = np.repeat(predict_arr, NPOISSON_SAMPS,\n",
        "                          axis=0).astype(np.float64)\n",
        "    psamps = np.random.poisson(all_means)\n",
        "    p95 = np.percentile(psamps, 95, axis=0)\n",
        "    p50 = np.percentile(psamps, 50, axis=0)\n",
        "    p05 = np.percentile(psamps, 5, axis=0)\n",
        "    return p05, p50, p95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc4wISpgDzIH"
      },
      "outputs": [],
      "source": [
        "def get_intervals_on_test(poisson_fit):\n",
        "    \"\"\"Get the quantiles by sampling from the Poisson distribution on the\n",
        "    test set.\n",
        "    \"\"\"\n",
        "\n",
        "    all_predict_cnts = []\n",
        "    for idx, minute_day in enumerate(X_test):\n",
        "        if idx > 0 and idx % REPORT == 0:\n",
        "          pass\n",
        "            # print(\"Sampling test dataset number\", idx)\n",
        "        # Wrap around back to end at day zero:\n",
        "        preds = poisson_fit.predict(minute_day)\n",
        "        # Now, let's get the predicted counts (poisson means) for this\n",
        "        # version of the test set:\n",
        "        predicted_cnts = preds  # Now the preds are the predictions\n",
        "        predict_cnts_arr = np.array(predicted_cnts)\n",
        "        all_predict_cnts.append(predict_cnts_arr)\n",
        "    predict_arr = np.array(all_predict_cnts)\n",
        "    p05, p50, p95 = get_quantiles(predict_arr)\n",
        "    return p05, p50, p95, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgGnjvkrD1l-"
      },
      "outputs": [],
      "source": [
        "def get_coverage(p05, p95, truth):\n",
        "    \"\"\"How often is the truth between p05 and p95.\"\"\"\n",
        "    ncovered = 0\n",
        "    ntot = 0\n",
        "    for yval, lower, upper in zip(truth, p05, p95):\n",
        "        ntot += 1\n",
        "        if lower <= yval <= upper:\n",
        "            ncovered += 1\n",
        "    return ncovered/ntot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfYFQGGMjwp-"
      },
      "outputs": [],
      "source": [
        "p05, p50, p95, y_test = get_intervals_on_test(poisson_fit)\n",
        "coverage = get_coverage(p05, p95, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzrRG2ZrMu9F",
        "outputId": "dc78e0f5-939d-427b-b9f4-92764d7b948f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOMok73Mz0Dw"
      },
      "source": [
        "# Functions Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72v5KeduzwY8"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdzvJLjs0BwW"
      },
      "outputs": [],
      "source": [
        "BOUND = \"|\"\n",
        "IGNORE_INDEX = -100\n",
        "ITEM_DATA_SEP = ','\n",
        "TRACE_DATA_SEP = \" \"\n",
        "\n",
        "FUNCTION_MODEL_NAME = 'function_model.pt'\n",
        "SEQ_LEN = 500\n",
        "BATCH_SIZE = 100\n",
        "MAX_ITERS = 10\n",
        "LR = 5e-3\n",
        "WEIGHT_DECAY = 1e-5 \n",
        "NLAYERS = 2\n",
        "NHIDDEN =  200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwf1ZgSM4VgB"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90cN0c6tHY5L"
      },
      "outputs": [],
      "source": [
        "class ExampleKeys(Enum):\n",
        "    \"\"\"Keys we can use to extract values from an example.\"\"\"\n",
        "    INPUT = \"input\"\n",
        "    TARGET = \"target\"\n",
        "    OUT_MASK = \"mask\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSs18ljBPqUF"
      },
      "outputs": [],
      "source": [
        "class FlavTensorMaker():\n",
        "    \"\"\"Make tensors for inputs and outputs, as requested, given list of all hash\n",
        "    functions with BOUND, which is used only to get list of flavors, which we turn into\n",
        "    a mapping from flavors to indexes in the features tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hash_functs): \n",
        "        \"\"\"The tensors depend on how many codes in the flav_map.\n",
        "        Args:\n",
        "        hash_functs: vector containing all hash functions and triggers of form\n",
        "        hashfunction_trigger.\n",
        "        \"\"\"\n",
        "        self.hfunc_idxs, self.idx_hfunc = self.get_hfunc_idxs(hash_functs)\n",
        "        self.ninput = len(self.hfunc_idxs)\n",
        "        # For timestamp feats:\n",
        "        self.ninput += 1440 + DAYS\n",
        "        self.noutput = len(self.hfunc_idxs)\n",
        "    \n",
        "\n",
        "    def get_ninput(self):\n",
        "        \"\"\"Getter for the ninput\n",
        "        \"\"\"\n",
        "        return self.ninput\n",
        "\n",
        "    def get_noutput(self):\n",
        "        \"\"\"Getter for the noutput\n",
        "        \"\"\"\n",
        "        return self.noutput\n",
        "\n",
        "    @staticmethod\n",
        "    def get_hfunc_idxs(hash_functs):\n",
        "        \"\"\"Return map from hashfunction_trigger to index, and one with reverse mapping,\n",
        "        using values in hash_functions list.\n",
        "        \"\"\"\n",
        "        hfunc_idx = {}\n",
        "        idx_hfunc = {}\n",
        "\n",
        "        for idx, hash_f in enumerate(hash_functs):\n",
        "            hfunc_idx[hash_f] = idx\n",
        "            idx_hfunc[idx] = hash_f\n",
        "    \n",
        "        return hfunc_idx, idx_hfunc\n",
        "\n",
        "    def __one_hot_hfunc_line(self, hash_func):\n",
        "        \"\"\"One-hot-encode line of hash of functions as a tensor of LEN(LINE) x 1 x NDIMS.\n",
        "        \"\"\"\n",
        "        ndims = len(self.hfunc_idxs)\n",
        "        hfvals = torch.tensor([self.hfunc_idxs[f] for f in hash_func])\n",
        "        tensor = torch.nn.functional.one_hot(hfvals,\n",
        "                                             num_classes=ndims) \\\n",
        "                                    .unsqueeze(dim=1)\n",
        "        return tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def one_hots_minute_day(minute_day):\n",
        "        \"\"\"Encode both minute-of-day (from 1 to 1440) and day-of-history (from 1 to\n",
        "        DAYS) using 1-hot ecoding and return 1440+DAYS-dimensional tensor.\n",
        "        This is public so we can re-use it in other classes\n",
        "        (e.g. features for Poisson Regression in narrivals).\n",
        "        \"\"\"\n",
        "        # We don't know what REAL day it was, but even if back at\n",
        "        # start of Linux, it's fine for finding patterns:\n",
        "        \n",
        "        tensor = torch.from_numpy(np.array(minute_day))\n",
        "        return tensor\n",
        "\n",
        "\n",
        "    def encode_input(self, minute_day, line):\n",
        "        \"\"\"Given line of N hashes of functions, output should be (Nhashfunction-1) x 1 x NFEATURES\n",
        "        [since last flav is not part of INPUT].\n",
        "        \"\"\"\n",
        "        input_line = line[:-1]\n",
        "        oneh_hfuncs = self.__one_hot_hfunc_line(input_line)\n",
        "        oneh_md = self.one_hots_minute_day(minute_day)\n",
        "        oneh_md_line = oneh_md.repeat(len(input_line), 1, 1)\n",
        "        \n",
        "        return torch.cat([oneh_hfuncs, oneh_md_line], dim=2)\n",
        "\n",
        "    def encode_target(self, hash_func):\n",
        "        \"\"\"For line of NFLAVS, return NFLAVS-1 targets giving indexes of the\n",
        "        true flavs.\n",
        "        \"\"\"\n",
        "        hfunc_idxs = [self.hfunc_idxs[hfunc] for hfunc in hash_func[1:]]\n",
        "        return torch.LongTensor(hfunc_idxs)\n",
        "\n",
        "    def replace_hfunc_input(self, my_input, new_hfunc):\n",
        "        \"\"\"Replace existing encoding of flavor in given input, in place, with\n",
        "        encoding of 'new_flav' instead.\n",
        "        \"\"\"\n",
        "        nhot_hfunc_feats = len(self.hfunc_idxs)\n",
        "        hfunc = torch.tensor([self.hfunc_idxs[new_hfunc]])\n",
        "        new_hfunc_tensor = torch.nn.functional.one_hot(\n",
        "            hfunc, num_classes=nhot_hfunc_feats)[0]\n",
        "        my_input[0, 0, :nhot_hfunc_feats] = new_hfunc_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkxivNJR55lE"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_vj5wPU8KtW"
      },
      "outputs": [],
      "source": [
        "class TraceLSTM(nn.Module):\n",
        "    \"\"\"Generic LSTM for hash functions - triggers modeling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ninput, nhidden, noutput, nlayers):\n",
        "        \"\"\"Depending on the size of the input, output, and the array of hidden\n",
        "        layers, add attributes for the inner LSTM and the\n",
        "        fully-connected layers (including both weights and a bias\n",
        "        term).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ninput = ninput\n",
        "        self.nhidden = nhidden\n",
        "        self.noutput = noutput\n",
        "        self.nlayers = nlayers\n",
        "        self.lstm = nn.LSTM(ninput, self.nhidden, self.nlayers)\n",
        "        self.fc_out = nn.Linear(self.nhidden, noutput)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self, device=None, batch_size=1):\n",
        "        \"\"\"Before doing each new sequence, re-init hidden state to zeros.\n",
        "        \"\"\"\n",
        "        hid0 = torch.zeros(self.nlayers, batch_size, self.nhidden)\n",
        "        c_hid0 = torch.zeros(self.nlayers, batch_size, self.nhidden)\n",
        "        if device is not None:\n",
        "            hid0 = hid0.to(device)\n",
        "            c_hid0 = c_hid0.to(device)\n",
        "        return (hid0, c_hid0)\n",
        "\n",
        "    def forward(self, minibatch):\n",
        "        \"\"\"Pass in a tensor of training examples of dimension LENGTH x\n",
        "        BATCHSIZE x NINPUT, then run the forward pass. Returns tensor\n",
        "        of LENGTH x BATCHSIZE x NOUTPUT.\n",
        "        \"\"\"\n",
        "        lstm_out, self.hidden = self.lstm(minibatch, self.hidden)\n",
        "        all_logits = self.fc_out(lstm_out)\n",
        "        return all_logits\n",
        "\n",
        "    def save(self, outfn):\n",
        "        \"\"\"Use the state-dict method of saving:\n",
        "        \"\"\"\n",
        "        torch.save(self.state_dict(), outfn)\n",
        "\n",
        "    @classmethod\n",
        "    def create_from_path(cls, filename, device=None):\n",
        "        \"\"\"Factory method to return an instance of this class, given the model\n",
        "        state-dict at the current filename. If device given,\n",
        "        dynamically move model to device.\n",
        "        \"\"\"\n",
        "        if device is not None:\n",
        "            torch_device = torch.device(device)\n",
        "            state_dict = torch.load(filename, map_location=torch_device)\n",
        "        else:\n",
        "            state_dict = torch.load(filename)\n",
        "        nhidden = state_dict['fc_out.weight'].shape[1]\n",
        "        noutput = state_dict['fc_out.weight'].shape[0]\n",
        "        # LSTM layers have 4 values: ih/hh weights and ih/hh biases:\n",
        "        nlayers = len(state_dict.keys()) // 4\n",
        "        ninput = state_dict['lstm.weight_ih_l0'].shape[1]\n",
        "        new_model = cls(ninput, nhidden, noutput, nlayers)\n",
        "        new_model.load_state_dict(state_dict)\n",
        "        return new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsNfKnkJAkxw"
      },
      "outputs": [],
      "source": [
        "class LossStats():\n",
        "    \"\"\"A class to hold, and reset as needed, the loss stats, during\n",
        "    training or testing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize all our running totals to zero.\"\"\"\n",
        "        self.tot_loss = 0\n",
        "        self.tot_examples = 0\n",
        "\n",
        "    def update(self, num, loss):\n",
        "        \"\"\"Given we've processed num examples, and observed an average loss of\n",
        "        loss, update our totals.\n",
        "        \"\"\"\n",
        "        if num == 0:\n",
        "            return\n",
        "        self.tot_loss += loss * num\n",
        "        self.tot_examples += num\n",
        "\n",
        "    def get_tot_examples(self):\n",
        "        \"\"\"Return total number of examples processed since beginning.\"\"\"\n",
        "        return self.tot_examples\n",
        "\n",
        "    def overall_loss(self):\n",
        "        \"\"\"Calculate and return the overall loss.\"\"\"\n",
        "        return self.tot_loss / self.tot_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YQqzIRb8KnT"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TrainArgs(NamedTuple):\n",
        "    \"\"\"Arguments to be used in training.\"\"\"\n",
        "    learn_rate: float\n",
        "    weight_decay: float\n",
        "    max_iters: int\n",
        "\n",
        "\n",
        "class TrainLSTM():\n",
        "    \"\"\"Class to handle flavor-LSTM training.\"\"\"\n",
        "    def __init__(self, eval_lstm, net, train_args, trainloader):\n",
        "        self.eval_lstm = eval_lstm\n",
        "        self.net = net\n",
        "        self.train_args = train_args\n",
        "        self.trainloader = trainloader\n",
        "\n",
        "    def run_train_iteration(self, data, optimizer, criterion):\n",
        "        \"\"\"Run a single training step and return the number of inputs\n",
        "        processed and the loss.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        num, loss = self.eval_lstm.batch_forward(data, criterion)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return num, loss\n",
        "\n",
        "    def iterate_models(self, optimizer, criterion):\n",
        "        \"\"\"Run a single training iteration and yield the loss\"\"\"\n",
        "        for epoch in range(self.train_args.max_iters):\n",
        "            self.net.train()\n",
        "            loss_stats = LossStats()\n",
        "            for iter_num, batch in enumerate(self.trainloader, 1):\n",
        "                num, loss = self.run_train_iteration(batch, optimizer,\n",
        "                                                     criterion)\n",
        "                loss_stats.update(num, loss)\n",
        "            overall_loss = loss_stats.overall_loss()\n",
        "            tot_examples = loss_stats.get_tot_examples()\n",
        "            logger.info('Train loss, epoch [%d, %7d]: %.7f',\n",
        "                        epoch, tot_examples, overall_loss)\n",
        "            yield overall_loss\n",
        "\n",
        "    def run(self, criterion):\n",
        "        \"\"\"Run training on the given neural network.\n",
        "        \"\"\"\n",
        "        optimizer = torch.optim.Adam(self.net.parameters(),\n",
        "                                     lr=self.train_args.learn_rate,\n",
        "                                     weight_decay=self.train_args.weight_decay)\n",
        "        print(\"Optimizer: \", optimizer)\n",
        "        print(\"Starting training\")\n",
        "        for iter_num, train_loss in enumerate(self.iterate_models(\n",
        "                optimizer, criterion), 1):\n",
        "            self.eval_lstm.get_test_score(iter_num, criterion)\n",
        "        print(\"Finished training\")\n",
        "        return self.net\n",
        "\n",
        "\n",
        "def get_init_model(args, tmaker):\n",
        "    \"\"\"Return an initial LSTM model for training, given the tensor\n",
        "    maker for this LSTM.\n",
        "    \"\"\"\n",
        "    # Get ndims from the tensor_maker for this functions hashes:\n",
        "    ninput = tmaker.get_ninput()\n",
        "    noutput = tmaker.get_noutput()\n",
        "    model = TraceLSTM(ninput, args.nhidden, noutput, args.nlayers)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABz2pfYd8I6P"
      },
      "source": [
        "## Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enqC-_NGzDwq"
      },
      "outputs": [],
      "source": [
        "def yield_trace_lines(trace_fn):    # trace_fn: whole dataframe\n",
        "  \"\"\"Read and yield data from the trace line-by-line: for either\n",
        "  flavors, or durations.\n",
        "  \"\"\"\n",
        "  for index, row in trace_fn.iterrows():\n",
        "    hash_functs = row['HashFunction'].split(',')\n",
        "    \n",
        "    yield row[[str(i) for i in range(1, 1441+DAYS)]].to_list(), hash_functs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK2NRwZwxPAs"
      },
      "outputs": [],
      "source": [
        "class FlavDataset(Dataset):\n",
        "    \"\"\"A dataset that can be used for flavor sequence modeling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seq_len, dataset_fn):    # dataset_fn = line\n",
        "        \"\"\"Initialize the dataset class.\n",
        "        Args:\n",
        "        flav_map_fn: String, filename with map from flavors to their codes.\n",
        "        seq_len: Int, how long to make the sequences for each example.\n",
        "        dataset_fn: String, filename where input dataset lies.\n",
        "        range_start/stop: Int: timestamps for start/end of training data.\n",
        "        \"\"\"\n",
        "        self.seq_len = seq_len\n",
        "        self.tmaker = FlavTensorMaker(hash_functions)   # takes hashes of functions\n",
        "        trace_data = yield_trace_lines(dataset_fn)  # takkes whole dataset\n",
        "        # make one giant example, getitem() & len() will take pieces:\n",
        "        self.all_inputs, self.all_targets = self.__make_example_tensor(\n",
        "            trace_data)\n",
        "        \n",
        "        assert len(self.all_inputs) == len(self.all_targets)\n",
        "\n",
        "    def __make_example_tensor(self, trace_data):    # trace_data = whole dataset\n",
        "        \"\"\"Go through lines in trace and create one big example tensor, where\n",
        "        the first dimension is example number.\n",
        "        \"\"\"\n",
        "        all_inputs, all_targets = [], []\n",
        "        for idx, line in enumerate(trace_data):\n",
        "            my_input, my_target = self.__make_example_from_line(line)\n",
        "            all_inputs.append(my_input)\n",
        "            all_targets.append(my_target)\n",
        "            if idx > 1 and idx % REPORT == 0:\n",
        "                logger.info(\"Read %s dataset lines\", idx)\n",
        "        logger.info(\"Read %s dataset lines\", idx)\n",
        "        # Create a single vector for each of these by reshaping:\n",
        "        all_inputs = torch.cat(all_inputs)\n",
        "        all_targets = torch.cat(all_targets)\n",
        "        all_inputs, all_targets = self.__reshape_data(\n",
        "            all_inputs, all_targets)\n",
        "        return all_inputs, all_targets\n",
        "\n",
        "    def __reshape_data(self, all_inputs, all_targets):\n",
        "        \"\"\"Depending on the sequence length, reshape accordingly.  Also, pad\n",
        "         with targets with IGNORE_INDEX so that we divide evenly.\n",
        "        \"\"\"\n",
        "        nflavs = len(all_inputs)\n",
        "        nseqs = ceil(1.0 * nflavs / self.seq_len)\n",
        "        padding_needed = nseqs * self.seq_len - nflavs\n",
        "        fake_targets = (torch.ones(padding_needed, dtype=torch.long) *\n",
        "                        IGNORE_INDEX)\n",
        "        all_targets = torch.cat([all_targets, fake_targets])\n",
        "        fake_input_shape = list(all_inputs.shape)\n",
        "        fake_input_shape[0] = padding_needed\n",
        "        fake_input = torch.zeros(fake_input_shape)\n",
        "        all_inputs = torch.cat([all_inputs, fake_input])\n",
        "        # After padding, reshape into sequences of seq_len:\n",
        "        reshaped_inputs = all_inputs.reshape(-1, self.seq_len, 1,\n",
        "                                             fake_input_shape[-1])\n",
        "        reshaped_targets = all_targets.reshape(-1, self.seq_len)\n",
        "        return reshaped_inputs, reshaped_targets\n",
        "\n",
        "    def __make_example_from_line(self, line):\n",
        "        \"\"\"Unpack the line and make the example from it.\n",
        "        \"\"\"\n",
        "        timestamp, flavs = line\n",
        "        # timestamp = int(timestamp)\n",
        "        # Lines don't BEGIN with BOUND, so put it on:\n",
        "        flavs = [BOUND] + flavs\n",
        "        ex_input = self.tmaker.encode_input(timestamp, flavs)\n",
        "        ex_target = self.tmaker.encode_target(flavs)\n",
        "        return ex_input, ex_target\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return number of sequences of length seq_len:\n",
        "        \"\"\"\n",
        "        return len(self.all_targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return an example from all our pre-made tensors.\"\"\"\n",
        "        ex_input = self.all_inputs[idx]\n",
        "        ex_target = self.all_targets[idx]\n",
        "        sample = {ExampleKeys.INPUT: ex_input,\n",
        "                  ExampleKeys.TARGET: ex_target}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxS1lvmXRY40"
      },
      "outputs": [],
      "source": [
        "class CollateUtils():\n",
        "    \"\"\"Utility collate functions for the DataLoaders.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def batching_collator(batch):\n",
        "        \"\"\"Return an example (dict) where the values are now minibatches.\n",
        "        Arguments: batch: an iterable over example (dicts) in a Dataset\n",
        "        Returns: collated, a single example with minibatch payload\n",
        "        \"\"\"\n",
        "        all_inputs = []\n",
        "        all_targets = []\n",
        "        all_masks = []\n",
        "        do_masks = batch[0].get(ExampleKeys.OUT_MASK) is not None\n",
        "        for example in batch:\n",
        "            all_inputs.append(example[ExampleKeys.INPUT])\n",
        "            # targets are either a flat SEQ_LEN vector of targets (in\n",
        "            # flavors) or 47 (in durs), so reshape accordingly:\n",
        "            targets = example[ExampleKeys.TARGET]\n",
        "            if len(targets.shape) == 1:\n",
        "                all_targets.append(targets.reshape(-1, 1, 1))\n",
        "            else:\n",
        "                all_targets.append(targets.reshape(-1, 1, targets.shape[-1]))\n",
        "            if do_masks:\n",
        "                masks = example[ExampleKeys.OUT_MASK]\n",
        "                all_masks.append(masks.reshape(-1, 1, targets.shape[-1]))\n",
        "        # Join them together along the batch dimension:\n",
        "        new_inputs = torch.cat(all_inputs, dim=1)\n",
        "        new_targets = torch.cat(all_targets, dim=1)\n",
        "        collated = {ExampleKeys.INPUT: new_inputs,\n",
        "                    ExampleKeys.TARGET: new_targets}\n",
        "        if do_masks:\n",
        "            new_masks = torch.cat(all_masks, dim=1)\n",
        "            collated[ExampleKeys.OUT_MASK] = new_masks\n",
        "        return collated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mElv4CkfYpSJ"
      },
      "outputs": [],
      "source": [
        "class Evaluator(ABC):\n",
        "    \"\"\"Class to run the forward pass and compute test scores.\"\"\"\n",
        "    def __init__(self, net, device_str, testloader):\n",
        "        self.net = net\n",
        "        self.device = torch.device(device_str)\n",
        "        self.testloader = testloader\n",
        "        if self.device.type == \"cuda\":\n",
        "            if self.device.index == 0:\n",
        "                self.net = self.net.cuda(0)\n",
        "            else:\n",
        "                self.net = self.net.cuda(1)\n",
        "\n",
        "    @abstractmethod\n",
        "    def batch_forward(self, batch, criterion):\n",
        "        \"\"\"Override to pick the outputs for the batch, compute the loss.\"\"\"\n",
        "\n",
        "    def get_test_score(self, epoch, criterion):\n",
        "        \"\"\"Get the score of current net on test set.\"\"\"\n",
        "        loss_stats = LossStats()\n",
        "        with torch.no_grad():\n",
        "            self.net.eval()\n",
        "            for iter_num, batch in enumerate(self.testloader, 1):\n",
        "                num, loss = self.batch_forward(batch, criterion)\n",
        "                loss_stats.update(num, loss)\n",
        "        overall_loss = loss_stats.overall_loss()\n",
        "        if epoch is not None:\n",
        "            print('Test loss, epoch {}: {:.7f}'.format(epoch, overall_loss))\n",
        "        else:\n",
        "            print('Test loss: {:.7f}', overall_loss)\n",
        "        return overall_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFANJLAFFZWw"
      },
      "outputs": [],
      "source": [
        "def make_hfunc_dataloaders(args):\n",
        "\n",
        "    try:\n",
        "        trainset = FlavDataset(args.seq_len, args.train_flavs)\n",
        "        trainloader = DataLoader(trainset, batch_size=args.batch_size,\n",
        "                                 collate_fn=CollateUtils.batching_collator,\n",
        "                                 shuffle=True)\n",
        "        \n",
        "    except AttributeError:\n",
        "        # No train_flavs provided:\n",
        "        trainloader = None\n",
        "    testset = FlavDataset(args.seq_len, args.test_flavs)\n",
        "    testloader = DataLoader(testset, batch_size=args.batch_size,\n",
        "                            collate_fn=CollateUtils.batching_collator,\n",
        "                            shuffle=False)\n",
        "    \n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "class EvaluateFlavLSTM(Evaluator):\n",
        "    \"\"\"Class to help with testing of a flavor LSTM.\"\"\"\n",
        "    def batch_forward(self, batch, criterion):\n",
        "        \"\"\"Run the forward pass and get the number of examples and the loss.\n",
        "        \"\"\"\n",
        "        inputs = batch[ExampleKeys.INPUT]\n",
        "        targets = batch[ExampleKeys.TARGET]\n",
        "        num = targets[targets != IGNORE_INDEX].numel()\n",
        "        inputs, targets = (inputs.to(self.device),\n",
        "                           targets.to(self.device))\n",
        "        batch_size = inputs.shape[1]\n",
        "        self.net.hidden = self.net.init_hidden(self.device, batch_size)\n",
        "        outputs = self.net(inputs)\n",
        "        outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "        targets = targets.reshape(-1)\n",
        "        loss = criterion(outputs, targets)\n",
        "        return num, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuBjlZK1EivD"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9ZzRze8Vq_"
      },
      "source": [
        "### Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CIsHaPkcl3R"
      },
      "outputs": [],
      "source": [
        "X = data.loc[:, [str(i) for i in range(1, 1441+DAYS)] + ['HashFunction']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=10, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0bAuvRC8inS"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKQTBUO1Gxb8"
      },
      "outputs": [],
      "source": [
        "args={\n",
        "    \"train_flavs\" : X_train,\n",
        "    \"test_flavs\" : X_test,\n",
        "    \"device\" : \"cpu\" , #,\"cuda:0\"\n",
        "    \"seq_len\" : SEQ_LEN,\n",
        "    \"batch_size\" : BATCH_SIZE,\n",
        "    \"max_iters\" : MAX_ITERS,\n",
        "    \"lr\" : LR,\n",
        "    \"weight_decay\" : WEIGHT_DECAY, \n",
        "    \"nlayers\" : NLAYERS,\n",
        "    \"nhidden\" :  NHIDDEN,\n",
        "    \"model_save_fn\" : DATA_PATH + FUNCTION_MODEL_NAME\n",
        "}\n",
        "\n",
        "# Convert Dict to object\n",
        "args = namedtuple(\"Args\", args.keys())(*args.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8skQb97_EieW",
        "outputId": "94c87b07-e119-4ad5-90b2-a2e5d1480604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer:  Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.005\n",
            "    weight_decay: 1e-05\n",
            ")\n",
            "Starting training\n",
            "Test loss, epoch 1: 2.7973819\n",
            "Test loss, epoch 2: 1.7216755\n",
            "Test loss, epoch 3: 1.7217933\n",
            "Test loss, epoch 4: 1.6459262\n",
            "Test loss, epoch 5: 1.5785716\n",
            "Test loss, epoch 6: 1.5376884\n",
            "Test loss, epoch 7: 1.5433611\n",
            "Test loss, epoch 8: 1.5797707\n",
            "Test loss, epoch 9: 1.5986450\n",
            "Test loss, epoch 10: 1.5921378\n",
            "Finished training\n"
          ]
        }
      ],
      "source": [
        "tmaker = FlavTensorMaker(hash_functions)\n",
        "net = get_init_model(args, tmaker)\n",
        "trainloader, testloader = make_hfunc_dataloaders(args)\n",
        "train_args = TrainArgs(args.lr, args.weight_decay, args.max_iters)\n",
        "eval_lstm = EvaluateFlavLSTM(net, args.device, testloader)\n",
        "train_run = TrainLSTM(eval_lstm, net, train_args, trainloader)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "trained_net = train_run.run(criterion)\n",
        "\n",
        "trained_net.save(args.model_save_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGgeTi413Bvy"
      },
      "source": [
        "## HashFunction Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gAK5rc_CR6D"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6e9pkszCUkJ"
      },
      "outputs": [],
      "source": [
        "OUTPUT_MODEL_NAME = 'output_model.pt'\n",
        "OUTPUT_DATA = 'output.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbygPIMu87lI"
      },
      "source": [
        "### Generator LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL2x25qg3Ely"
      },
      "outputs": [],
      "source": [
        "class GenLSTM():\n",
        "    \"\"\"An evaluator that only does forward pass (no loss calculation).\"\"\"\n",
        "    def __init__(self, net, device):\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "\n",
        "    def init_hidden(self):\n",
        "        self.net.hidden = self.net.init_hidden(self.device)\n",
        "\n",
        "    def forward(self, my_input):\n",
        "        my_input = my_input.float()\n",
        "        outputs = self.net(my_input)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-2asHoyAISN"
      },
      "outputs": [],
      "source": [
        "def make_arrival_vector(x):\n",
        "  return x\n",
        "\n",
        "def encode_trace_line(timestamp, item_lst):\n",
        "    itemstr = ITEM_DATA_SEP.join(item_lst)\n",
        "    out_str = \"{}{}{}\".format(timestamp, TRACE_DATA_SEP, itemstr)\n",
        "    return out_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIwSyHN83Gjq"
      },
      "outputs": [],
      "source": [
        "class Generator():\n",
        "    \"\"\"Creates an object that generates a trace (flavs and durs) according\n",
        "    to our batching baseline trace generator process.\n",
        "    \"\"\"\n",
        "    def __init__(self, device, arrival_mdl, hfunc_lstm):\n",
        "        \"\"\"arrival_mdl: the Poisson GLM mdl from statsmodels\n",
        "        hfunc_lstm/dur_lstm: LSTM evaluators to run forward passes\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.arrival_mdl = arrival_mdl\n",
        "        self.hfunc_to_idxs, self.idx_to_hfuncs = FlavTensorMaker.get_hfunc_idxs(hash_functions)\n",
        "        \n",
        "        self.hfunc_tmaker = FlavTensorMaker(hash_functions)\n",
        "        \n",
        "        self.hfunc_lstm = hfunc_lstm\n",
        "        \n",
        "    def __get_narrivals(self, timestamp):\n",
        "        pred_mean = self.arrival_mdl.predict([timestamp])[0]\n",
        "        narrivals = np.random.poisson(pred_mean)\n",
        "        return narrivals\n",
        "\n",
        "    def __init_hfunc_input(self, timestamp):\n",
        "        \"\"\"Initialize input tensor to a BOUND at given timestamp.\"\"\"\n",
        "        hfunc_lst = [BOUND, BOUND]  # second BOUND ignored by tmaker\n",
        "        my_input = self.hfunc_tmaker.encode_input(timestamp.tolist(), hfunc_lst)\n",
        "        my_input = my_input.to(self.device)\n",
        "        return my_input\n",
        "\n",
        "    def __adjust_hfunc_input(self, my_input, prev_hfunc):\n",
        "        \"\"\"Replace the flavor part of the input only.\"\"\"\n",
        "        self.hfunc_tmaker.replace_hfunc_input(my_input, prev_hfunc)\n",
        "        return my_input\n",
        "\n",
        "    def __sample_hfunc(self, output):\n",
        "        \"\"\"Sample flavor from output of flavor LSTM.\"\"\"\n",
        "        probs = torch.softmax(output.reshape(-1), dim=0)\n",
        "        hfunc_idx = torch.multinomial(probs, 1).item()\n",
        "        # Also get flavor string itself:\n",
        "        hfunc_hfunc = self.idx_to_hfuncs[hfunc_idx]\n",
        "        return hfunc_hfunc, hfunc_idx\n",
        "\n",
        "    def __generate_hfunc(self, timestamp, target_nbatches):\n",
        "        \"\"\"Auto-regressively generate target_nbatches batches of flavors, at\n",
        "        given timestamp.\n",
        "        \"\"\"\n",
        "        my_input = self.__init_hfunc_input(timestamp)\n",
        "        hfunc_hfunc = []\n",
        "        hfunc_idxs = []\n",
        "        nseen_batches = 0\n",
        "        prev_hfunc = BOUND\n",
        "        while True:\n",
        "            output = self.hfunc_lstm.forward(my_input)\n",
        "            next_hfunc, next_idx = self.__sample_hfunc(output)\n",
        "            # Shouldn't happen, but skip if it does:\n",
        "            if next_hfunc == BOUND and prev_hfunc == BOUND:\n",
        "                continue\n",
        "            hfunc_hfunc.append(next_hfunc)\n",
        "            hfunc_idxs.append(next_idx)\n",
        "            # Increment number batches seen on each bound:\n",
        "            if next_hfunc == BOUND:\n",
        "                nseen_batches += 1\n",
        "                if nseen_batches == target_nbatches:\n",
        "                    break\n",
        "            # Otherwise, get next input and continue:\n",
        "            my_input = self.__adjust_hfunc_input(my_input, next_hfunc)\n",
        "            prev_hfunc = next_hfunc\n",
        "        return hfunc_hfunc, hfunc_idxs\n",
        "\n",
        "\n",
        "    def __generate_batches(self, timestamp, nbatches):\n",
        "        \"\"\"Use flav/dur LSTMs to generate trace for a single row/timestamp.\"\"\"\n",
        "        hfunc_hfunc, hfunc_idxs = self.__generate_hfunc(timestamp, nbatches)\n",
        "        return hfunc_idxs\n",
        "\n",
        "    def __output_hfuncs(self, timestamp, hfunc_idxs, out_hfunc_file):\n",
        "        \"\"\"Make and output the flavor line.\"\"\"\n",
        "        hfunc_lst = [self.idx_to_hfuncs[i] for i in hfunc_idxs]\n",
        "        hfunc_out = encode_trace_line(timestamp, hfunc_lst)\n",
        "        print(hfunc_out, file=out_hfunc_file)\n",
        "\n",
        "\n",
        "    def __output_zero_arrivals(self, timestamp, out_hfuncs_file):\n",
        "        hfunc_out = encode_trace_line(timestamp, [])\n",
        "        print(hfunc_out, file=out_hfuncs_file)\n",
        "\n",
        "\n",
        "\n",
        "    def __call__(self, out_hfuncs_file):\n",
        "        \"\"\"Generate trace for timestamps from start_s to stop_s inclusive.\"\"\"\n",
        "        self.hfunc_lstm.init_hidden()\n",
        "        for ntimestamps, timestamp in enumerate(args.minute_day):\n",
        "            nbatches = self.__get_narrivals(timestamp)\n",
        "            if nbatches == 0:\n",
        "                self.__output_zero_arrivals(timestamp, out_hfuncs_file)\n",
        "                continue\n",
        "            hfunc_idxs = self.__generate_batches(timestamp, nbatches)\n",
        "            self.__output_hfuncs(timestamp, hfunc_idxs, out_hfuncs_file)\n",
        "            if ntimestamps > 0 and ntimestamps % REPORT == 0:\n",
        "                logger.info(\"Generated %d output lines (now on %d)\", ntimestamps, timestamp)\n",
        "\n",
        "\n",
        "def get_lstm_eval(device, model_fn):\n",
        "    \"\"\"Initialize the generation LSTM evaluator.\"\"\"\n",
        "    net = TraceLSTM.create_from_path(model_fn, device)\n",
        "    return GenLSTM(net, device)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    logger.info(\"Reading models\")\n",
        "    arrival_mdl = sm.load(args.arrival_model_pkl)\n",
        "    hfunc_gen = get_lstm_eval(args.device, args.flav_model)\n",
        "    lstm_generator = Generator(args.device, arrival_mdl, hfunc_gen)\n",
        "    with open(args.out_flavs_fn, \"w\") as hfuncs_file:\n",
        "        logger.info(\"Running generation\")\n",
        "        lstm_generator(hfuncs_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVFmZxVt9Eh9"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-ET2gBcyDnJ"
      },
      "outputs": [],
      "source": [
        "args={\n",
        "    \"arrival_model_pkl\" : DATA_PATH + ARRIVAL_MODEL_NAME,\n",
        "    \"device\" : \"cpu\" ,\n",
        "    \"flav_model\" : DATA_PATH + FUNCTION_MODEL_NAME,\n",
        "    \"out_flavs_fn\" : OUTPUT_DATA,\n",
        "    \"minute_day\": X_test[[str(i) for i in range(1, 1441+DAYS)]].to_numpy()\n",
        "}\n",
        "\n",
        "# Convert Dict to object\n",
        "args = namedtuple(\"Args\", args.keys())(*args.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0l6epTG_AMA"
      },
      "outputs": [],
      "source": [
        "main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZEnsRAFO-bt"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krXojuxRO-GW"
      },
      "outputs": [],
      "source": [
        "output_data =pd.read_csv(DATA_PATH + OUTPUT_DATA, delimiter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8nPQqljPyOY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gF9d5GUkUwA9",
        "nYUTZDHppAGO",
        "iGgeTi413Bvy",
        "cVFmZxVt9Eh9"
      ],
      "name": "nb_faas_trace_generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
